{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install pytesseract transformers datasets evaluate rouge-score nltk tensorboard py7zr --upgrade\n!pip install pytesseract transformers==4.28.1 datasets evaluate rouge-score nltk tensorboard py7zr ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-30T17:00:26.130569Z","iopub.execute_input":"2024-04-30T17:00:26.131435Z","iopub.status.idle":"2024-04-30T17:01:09.059604Z","shell.execute_reply.started":"2024-04-30T17:00:26.131400Z","shell.execute_reply":"2024-04-30T17:01:09.058332Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pytesseract in /opt/conda/lib/python3.10/site-packages (0.3.10)\nCollecting transformers==4.28.1\n  Downloading transformers-4.28.1-py3-none-any.whl.metadata (109 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.0/110.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nCollecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.15.1)\nCollecting py7zr\n  Downloading py7zr-0.21.0-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1) (2.31.0)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.1)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1) (4.66.1)\nRequirement already satisfied: Pillow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from pytesseract) (9.5.0)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.2)\nRequirement already satisfied: texttable in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.7.0)\nCollecting pycryptodomex>=3.16.0 (from py7zr)\n  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\nCollecting pyzstd>=0.15.9 (from py7zr)\n  Downloading pyzstd-0.15.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\nCollecting pyppmd<1.2.0,>=1.1.0 (from py7zr)\n  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\nCollecting pybcj<1.1.0,>=1.0.0 (from py7zr)\n  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting multivolumefile>=0.2.3 (from py7zr)\n  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\nCollecting inflate64<1.1.0,>=1.0.0 (from py7zr)\n  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting brotli>=1.1.0 (from py7zr)\n  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from py7zr) (5.9.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.28.1) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.1) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\nDownloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading py7zr-0.21.0-py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\nDownloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyzstd-0.15.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (411 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.2/411.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=bd3cfe1bf949bb8e906da4b3a1b4daa93d0fc2d29bca3d0e432fb1dde1e313a6\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: tokenizers, brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, rouge-score, py7zr, transformers, evaluate\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: brotli\n    Found existing installation: Brotli 1.0.9\n    Uninstalling Brotli-1.0.9:\n      Successfully uninstalled Brotli-1.0.9\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.39.3\n    Uninstalling transformers-4.39.3:\n      Successfully uninstalled transformers-4.39.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.28.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed brotli-1.1.0 evaluate-0.4.2 inflate64-1.0.0 multivolumefile-0.2.3 py7zr-0.21.0 pybcj-1.0.2 pycryptodomex-3.20.0 pyppmd-1.1.0 pyzstd-0.15.10 rouge-score-0.1.2 tokenizers-0.13.3 transformers-4.28.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Token id:\n# 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\nfrom huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-04-30T17:01:31.357006Z","iopub.execute_input":"2024-04-30T17:01:31.357411Z","iopub.status.idle":"2024-04-30T17:01:31.665514Z","shell.execute_reply.started":"2024-04-30T17:01:31.357373Z","shell.execute_reply":"2024-04-30T17:01:31.664465Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e4464a39c0146a8a8400841f90f06b9"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport re\nimport glob\nfrom datasets import load_dataset\nimport datasets\ndataset_id = \"imdb\"","metadata":{"execution":{"iopub.status.busy":"2024-04-30T19:53:29.098819Z","iopub.execute_input":"2024-04-30T19:53:29.099138Z","iopub.status.idle":"2024-04-30T19:53:31.054610Z","shell.execute_reply.started":"2024-04-30T19:53:29.099111Z","shell.execute_reply":"2024-04-30T19:53:31.053702Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load dataset from the hub\ndataset = load_dataset(dataset_id)\n\nprint(f\"Train dataset size: {len(dataset['train'])}\")\nprint(f\"Test dataset size: {len(dataset['test'])}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-30T19:53:42.933897Z","iopub.execute_input":"2024-04-30T19:53:42.934449Z","iopub.status.idle":"2024-04-30T19:54:06.589988Z","shell.execute_reply.started":"2024-04-30T19:53:42.934418Z","shell.execute_reply":"2024-04-30T19:54:06.589046Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f2099780dc444d8adeae1277beccdf1"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 21.0M/21.0M [00:00<00:00, 28.6MB/s]\nDownloading data: 100%|██████████| 20.5M/20.5M [00:00<00:00, 28.7MB/s]\nDownloading data: 100%|██████████| 42.0M/42.0M [00:00<00:00, 43.5MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38fd356842c34d968007c97573d6b041"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b560e7064164fe68ec61954d1e7db9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dce3f15a62a841cfa66de6092bef76d6"}},"metadata":{}},{"name":"stdout","text":"Train dataset size: 25000\nTest dataset size: 25000\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-30T17:01:58.169307Z","iopub.execute_input":"2024-04-30T17:01:58.170213Z","iopub.status.idle":"2024-04-30T17:01:58.178165Z","shell.execute_reply.started":"2024-04-30T17:01:58.170170Z","shell.execute_reply":"2024-04-30T17:01:58.176897Z"},"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"dataset['train'][1]","metadata":{"execution":{"iopub.status.busy":"2024-04-30T17:01:58.179609Z","iopub.execute_input":"2024-04-30T17:01:58.179926Z","iopub.status.idle":"2024-04-30T17:01:58.445955Z","shell.execute_reply.started":"2024-04-30T17:01:58.179900Z","shell.execute_reply":"2024-04-30T17:01:58.445048Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'text': '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.',\n 'label': 0}"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_id=\"google/flan-t5-base\"\n\n# Load tokenizer of FLAN-t5-base\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T17:01:58.448243Z","iopub.execute_input":"2024-04-30T17:01:58.448867Z","iopub.status.idle":"2024-04-30T17:02:00.685959Z","shell.execute_reply.started":"2024-04-30T17:01:58.448836Z","shell.execute_reply":"2024-04-30T17:02:00.684954Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3000e49b74c41ff80e9eff7d3454093"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3ddf943a09444569f5469f20a6952d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38f2960d0b7b4d30841abb347f9805d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3aaa120241234de9813ced1c9cb1f386"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nimport random\n\n# dataset['train'] = dataset['train'].shuffle(seed=42).select(range(2000)) \n# dataset['test'] = dataset['test'].shuffle(seed=42).select(range(1000)) \n\ndataset['train'] = dataset['train'].shuffle(seed=42)\n\ntrain_df = pd.DataFrame(dataset['train'][:1000])\ntest_df = pd.DataFrame(dataset['test'][:1000])\ndataset.clear()\ntrain_df['label'] = train_df['label'].astype(str)\ntest_df['label'] = test_df['label'].astype(str)\ndataset['train'] = Dataset.from_pandas(train_df)\ndataset['test'] = Dataset.from_pandas(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T17:02:13.069001Z","iopub.execute_input":"2024-04-30T17:02:13.070290Z","iopub.status.idle":"2024-04-30T17:02:13.156570Z","shell.execute_reply.started":"2024-04-30T17:02:13.070247Z","shell.execute_reply":"2024-04-30T17:02:13.155728Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-30T17:02:20.168025Z","iopub.execute_input":"2024-04-30T17:02:20.168902Z","iopub.status.idle":"2024-04-30T17:02:20.180409Z","shell.execute_reply.started":"2024-04-30T17:02:20.168865Z","shell.execute_reply":"2024-04-30T17:02:20.178706Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 1000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\n# The maximum total input sequence length after tokenization. \n# Sequences longer than this will be truncated, sequences shorter will be padded.\ntokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"text\"], truncation=True), batched=True, remove_columns=['text', 'label'])\nmax_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\nprint(f\"Max source length: {max_source_length}\")\n\n# The maximum total sequence length for target text after tokenization. \n# Sequences longer than this will be truncated, sequences shorter will be padded.\"\ntokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"label\"], truncation=True), batched=True, remove_columns=['text', 'label'])\nmax_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\nprint(f\"Max target length: {max_target_length}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-30T17:02:24.640560Z","iopub.execute_input":"2024-04-30T17:02:24.640939Z","iopub.status.idle":"2024-04-30T17:02:26.231793Z","shell.execute_reply.started":"2024-04-30T17:02:24.640910Z","shell.execute_reply":"2024-04-30T17:02:26.230772Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26e6350800ce4f628a749a5ca4fcdad8"}},"metadata":{}},{"name":"stdout","text":"Max source length: 512\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5959f4807f014b6ab5e81deac5054415"}},"metadata":{}},{"name":"stdout","text":"Max target length: 3\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def preprocess_function(sample, padding=\"max_length\"):\n    # add prefix to the input for t5\n    inputs = [item for item in sample[\"text\"]]\n\n    # tokenize inputs\n    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n\n    # Tokenize targets with the `text_target` keyword argument\n    labels = tokenizer(text_target=sample[\"label\"], max_length=max_target_length, padding=padding, truncation=True)\n    \n    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n    # padding in the loss.\n    if padding == \"max_length\":\n        labels[\"input_ids\"] = [\n            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n        ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['text', 'label'])\nprint(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-30T17:02:31.908070Z","iopub.execute_input":"2024-04-30T17:02:31.908467Z","iopub.status.idle":"2024-04-30T17:02:33.347792Z","shell.execute_reply.started":"2024-04-30T17:02:31.908428Z","shell.execute_reply":"2024-04-30T17:02:33.346798Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88a163f0b67747bdbb76f690f05fd7a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee290bd37aa24334902ba413b12bc04e"}},"metadata":{}},{"name":"stdout","text":"Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM\n\n# huggingface hub model id\nmodel_id=\"google/flan-t5-base\"\n\n# load model from the hub\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T17:02:36.732668Z","iopub.execute_input":"2024-04-30T17:02:36.733053Z","iopub.status.idle":"2024-04-30T17:03:18.340304Z","shell.execute_reply.started":"2024-04-30T17:02:36.733020Z","shell.execute_reply":"2024-04-30T17:03:18.339018Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"033eb3a70ea44373b0431252392609bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e845d99dff4a44d18f9eef5b9c7f1e3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ac2b1f4ce874cf6b28669cc3f2e4e66"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import evaluate\nimport nltk\nimport numpy as np\nfrom nltk.tokenize import sent_tokenize\nnltk.download(\"punkt\")\n\n# Metric\nmetric = evaluate.load(\"f1\")\n\n# helper function to postprocess text\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n\n    # rougeLSum expects newline after each sentence\n    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n\n    return preds, labels\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, average='macro')\n    result = {k: round(v * 100, 4) for k, v in result.items()}\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    return result\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T17:03:18.343041Z","iopub.execute_input":"2024-04-30T17:03:18.343899Z","iopub.status.idle":"2024-04-30T17:03:33.047211Z","shell.execute_reply.started":"2024-04-30T17:03:18.343855Z","shell.execute_reply":"2024-04-30T17:03:33.046305Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2024-04-30 17:03:21.415135: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-30 17:03:21.415240: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-30 17:03:21.617750: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f3a86620c3a4372ad41081d78bbb05e"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\n# we want to ignore tokenizer pad token in the loss\nlabel_pad_token_id = -100\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=label_pad_token_id,\n    pad_to_multiple_of=8\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T17:03:33.048397Z","iopub.execute_input":"2024-04-30T17:03:33.048701Z","iopub.status.idle":"2024-04-30T17:03:33.054073Z","shell.execute_reply.started":"2024-04-30T17:03:33.048675Z","shell.execute_reply":"2024-04-30T17:03:33.053141Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from huggingface_hub import HfFolder\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\n# Hugging Face repository id\nrepository_id = f\"{model_id.split('/')[1]}-imdb-text-classification\"\n\n# Define training args\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=repository_id,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    predict_with_generate=True,\n    fp16=False, # Overflows with fp16\n    learning_rate=3e-4,\n\n    num_train_epochs = 5,\n    # logging & evaluation strategies\n    logging_dir=f\"{repository_id}/logs\",\n    logging_strategy=\"epoch\", \n    # logging_steps=1000,\n    evaluation_strategy=\"no\",\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    load_best_model_at_end=False,\n    # metric_for_best_model=\"overall_f1\",\n    # push to hub parameters\n    report_to=\"tensorboard\",\n    push_to_hub=True,\n    hub_strategy=\"every_save\",\n    hub_model_id=repository_id,\n    hub_token=HfFolder.get_token(),\n)\n\n# Create Trainer instance\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T17:03:33.056439Z","iopub.execute_input":"2024-04-30T17:03:33.056826Z","iopub.status.idle":"2024-04-30T17:03:38.143256Z","shell.execute_reply.started":"2024-04-30T17:03:33.056791Z","shell.execute_reply":"2024-04-30T17:03:38.142053Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\nFor more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n  warnings.warn(warning_message, FutureWarning)\nCloning https://huggingface.co/amitku123/flan-t5-base-imdb-text-classification into local empty directory.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Start training \ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-30T17:03:38.144812Z","iopub.execute_input":"2024-04-30T17:03:38.145362Z","iopub.status.idle":"2024-04-30T17:13:42.847838Z","shell.execute_reply.started":"2024-04-30T17:03:38.145322Z","shell.execute_reply":"2024-04-30T17:13:42.846286Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [625/625 10:02, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>125</td>\n      <td>0.234600</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.101500</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.027800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.007300</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>0.004300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=625, training_loss=0.07512029418945312, metrics={'train_runtime': 604.6698, 'train_samples_per_second': 8.269, 'train_steps_per_second': 1.034, 'total_flos': 3423786762240000.0, 'train_loss': 0.07512029418945312, 'epoch': 5.0})"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Save our tokenizer and create model card\ntokenizer.save_pretrained(repository_id)\ntrainer.create_model_card()\n\n# Push the results to the hub\ntrainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-04-30T17:13:42.850073Z","iopub.execute_input":"2024-04-30T17:13:42.850440Z","iopub.status.idle":"2024-04-30T17:14:21.152076Z","shell.execute_reply.started":"2024-04-30T17:13:42.850411Z","shell.execute_reply":"2024-04-30T17:14:21.151056Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Several commits (2) will be pushed upstream.\nThe progress bars may be unreliable.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Upload file pytorch_model.bin:   0%|          | 1.00/945M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d723123f0c424ecdb7ec08aa274901d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload file spiece.model:   0%|          | 1.00/773k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76b979e240f644ab895e472e4a491807"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload file logs/events.out.tfevents.1714496618.06a913962fd8.34.0:   0%|          | 1.00/6.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87b4a9b22ba44526987a8ec699f703ab"}},"metadata":{}},{"name":"stderr","text":"To https://huggingface.co/amitku123/flan-t5-base-imdb-text-classification\n   85da2f6..3321340  main -> main\n\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'https://huggingface.co/amitku123/flan-t5-base-imdb-text-classification/commit/33213400ec874fa1ccc7f361588adf572b5a8980'"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nsamples_number = len(dataset['test'])\nprogress_bar = tqdm(range(samples_number))\npredictions_list = []\nlabels_list = []\nfor i in range(samples_number):\n  text = dataset['test']['text'][i]\n  inputs = tokenizer.encode_plus(text, padding='max_length', max_length=512, return_tensors='pt').to('cuda')\n  outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=150, num_beams=4, early_stopping=True)\n  prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n  predictions_list.append(prediction)\n  labels_list.append(dataset['test']['label'][i])\n\n  progress_bar.update(1)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T18:16:58.641010Z","iopub.execute_input":"2024-04-30T18:16:58.641840Z","iopub.status.idle":"2024-04-30T18:18:57.914275Z","shell.execute_reply.started":"2024-04-30T18:16:58.641804Z","shell.execute_reply":"2024-04-30T18:18:57.913246Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84d1566ec2d544e094e6bff244786e67"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"str_labels_list = []\nfor i in range(len(labels_list)): str_labels_list.append(str(labels_list[i]))","metadata":{"execution":{"iopub.status.busy":"2024-04-30T18:18:57.916031Z","iopub.execute_input":"2024-04-30T18:18:57.916383Z","iopub.status.idle":"2024-04-30T18:18:57.921876Z","shell.execute_reply.started":"2024-04-30T18:18:57.916353Z","shell.execute_reply":"2024-04-30T18:18:57.920779Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nreport = classification_report(str_labels_list, predictions_list, zero_division=0)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T18:18:57.923370Z","iopub.execute_input":"2024-04-30T18:18:57.924058Z","iopub.status.idle":"2024-04-30T18:18:57.961216Z","shell.execute_reply.started":"2024-04-30T18:18:57.924018Z","shell.execute_reply":"2024-04-30T18:18:57.960158Z"},"trusted":true},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           \"       0.00      0.00      0.00         0\n           (       0.00      0.00      0.00         0\n           0       1.00      0.86      0.93      1000\n           1       0.00      0.00      0.00         0\n\n    accuracy                           0.86      1000\n   macro avg       0.25      0.22      0.23      1000\nweighted avg       1.00      0.86      0.93      1000\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}